{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import _pickle as pkl\n",
    "from numpy import *\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑选了LDL里面的aa_knn，较为简单，不知道会不会影响结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa_knn(trainFeature, trainLabel, testFeature, testLabel, k):\n",
    "    rows = testFeature.shape[0]\n",
    "    cols = testFeature.shape[1]\n",
    "\n",
    "    train_num = trainFeature.shape[0]\n",
    "    dist = np.zeros(shape=(train_num))\n",
    "    prediction = np.zeros(shape=(rows,trainLabel.shape[1]))\n",
    "\n",
    "    for i in range(rows):\n",
    "        test = testFeature[i,:]\n",
    "        for j in range(train_num):\n",
    "            train = trainFeature[j,:]\n",
    "            V = test - train\n",
    "            sqV = V**2\n",
    "            sqDis = sqV.sum(axis=0)\n",
    "            dist[j] = math.sqrt(sqDis)\n",
    "\n",
    "        indice = dist.argsort()\n",
    "        sort_dis = sorted(dist)\n",
    "        sum_label = np.zeros(shape=trainLabel.shape[1])\n",
    "        for l in range(k):\n",
    "            sum_label = sum_label + trainLabel[l,:]\n",
    "\n",
    "        prediction[i,:] = sum_label/k\n",
    "    \n",
    "    idx = prediction.argmax(axis=1)\n",
    "    prediction = (idx[:,None] == np.arange(prediction.shape[1])).astype(int)\n",
    "\n",
    "    score = metrics.accuracy_score(prediction, testLabel)\n",
    "\n",
    "    return prediction, score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmc_error(mem):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmc_shap(tolerance, trainFeature, trainLabel, testFeature, testLabel, k, iterations, sources=None):\n",
    "    if sources is None:\n",
    "        sources = {i: np.array([i]) for i in range(len(trainFeature))}\n",
    "    elif not isinstance(sources, dict):\n",
    "        sources = {i: np.where(sources == i)[0] for i in set(sources)}\n",
    "\n",
    "    marginals = []\n",
    "    mem_tmc = np.zeros((0,len(trainFeature)))\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        idxs = np.random.permutation(len(sources))\n",
    "        marginal_contribs = np.zeros(len(trainFeature))\n",
    "\n",
    "        #x_batch = np.zeros((0,) + tuple(trainFeature.shape[1:]))\n",
    "        #y_batch = np.zeros(0, int)\n",
    "        x_batch = trainFeature[idxs[0]]\n",
    "        y_batch = trainLabel[idxs[0]]\n",
    "\n",
    "        truncation_counter = 0\n",
    "\n",
    "        #源代码里面是随机抽取测试数据，进行100次实验获得到的平均socre，这里直接设置为零\n",
    "        new_score = 0\n",
    "  \n",
    "        for sh, idx in enumerate(idxs):\n",
    "            old_score = new_score\n",
    "\n",
    "            if sh == 0 :\n",
    "                a=0\n",
    "            else:\n",
    "                x_batch = np.vstack((x_batch, trainFeature[idx]))\n",
    "                y_batch = np.vstack((y_batch, trainLabel[idx]))\n",
    "\n",
    "            if x_batch.shape[0] >= 2:\n",
    "                _, new_score = aa_knn(trainFeature, trainLabel, testFeature, testLabel, k)\n",
    "\n",
    "            marginal_contribs[sources[idx]] = (new_score - old_score)/len(sources[idx])\n",
    "            mean_score = 0.18\n",
    "\n",
    "            distance_to_full_score = np.abs(new_score - mean_score)\n",
    "            if distance_to_full_score <= tolerance * mean_score:\n",
    "                truncation_counter += 1\n",
    "                if truncation_counter > 5:\n",
    "                    break\n",
    "            else:\n",
    "                truncation_counter = 0\n",
    "        \n",
    "        mem_tmc = np.concatenate([mem_tmc, np.reshape(marginal_contribs, (1,-1))])\n",
    "\n",
    "    value_tmc = np.mean(mem_tmc,0)\n",
    "\n",
    "    return value_tmc,mem_tmc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_feature = open(\"ldlFeature.txt\",'r')\n",
    "feature = []\n",
    "\n",
    "for line in file_feature:\n",
    "    one_fea = list(map(lambda x : float(x), line.split()))\n",
    "    feature.append(one_fea)\n",
    "\n",
    "\n",
    "file_label = open(\"binaryLabel.txt\",'r')\n",
    "label = []\n",
    "\n",
    "for line in file_label:\n",
    "    one_label = list(map(lambda x : float(x), line.split()))\n",
    "    label.append(one_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置训练数据和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeature = array(feature[:100])\n",
    "trainLabel = array(label[:100])\n",
    "\n",
    "testFeature = array(feature[170:])\n",
    "testLabel = array(label[170:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap,tmc = tmc_shap(0.1, trainFeature, trainLabel, testFeature, testLabel, 5, 20, sources=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.00297297 0.00237838 0.00356757 0.00267568 0.00237838 0.00148649\n 0.00356757 0.00356757 0.00386486 0.00237838 0.00237838 0.00267568\n 0.00267568 0.00208108 0.00445946 0.00267568 0.00267568 0.00505405\n 0.00327027 0.00356757 0.00386486 0.00237838 0.00267568 0.00356757\n 0.00327027 0.00356757 0.00237838 0.00356757 0.00327027 0.00386486\n 0.00178378 0.00683784 0.00327027 0.00356757 0.00208108 0.00267568\n 0.00208108 0.00118919 0.00356757 0.00178378 0.00445946 0.00356757\n 0.00327027 0.00267568 0.00118919 0.00237838 0.00267568 0.00327027\n 0.00475676 0.00267568 0.00237838 0.00237838 0.00297297 0.00297297\n 0.00297297 0.00475676 0.00297297 0.00327027 0.00178378 0.00267568\n 0.00297297 0.00356757 0.00148649 0.00356757 0.00148649 0.00297297\n 0.00356757 0.00089189 0.00297297 0.00327027 0.00208108 0.00386486\n 0.00386486 0.00564865 0.00297297 0.00267568 0.00416216 0.00237838\n 0.00178378 0.00148649 0.00237838 0.00237838 0.00297297 0.00327027\n 0.00267568 0.00297297 0.00327027 0.00386486 0.00237838 0.00267568\n 0.00327027 0.00267568 0.00356757 0.00237838 0.00445946 0.00297297\n 0.00386486 0.00267568 0.00178378 0.00148649]\n0.005945945945945946 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n5\n0.002972972972972973 0.0\n0.0 0.0\n0.0 0.0\n0.0 0.0\n0.0 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.002972972972972973 0.0\n10\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n7\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n4\n0.002972972972972973 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.0 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.0 0.005945945945945946\n0.0 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.0 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n2\n0.0 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.0 0.002972972972972973\n0.008918918918918918 0.002972972972972973\n7\n0.008918918918918918 0.008918918918918918\n0.008918918918918918 0.008918918918918918\n0.0 0.008918918918918918\n0.0 0.008918918918918918\n0.002972972972972973 0.008918918918918918\n0.005945945945945946 0.008918918918918918\n0.002972972972972973 0.008918918918918918\n0.005945945945945946 0.008918918918918918\n0.0 0.008918918918918918\n0.002972972972972973 0.008918918918918918\n2\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n8\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n6\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n8\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.0 0.002972972972972973\n5\n0.0 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.005945945945945946 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.002972972972972973 0.0\n0.005945945945945946 0.0\n0.005945945945945946 0.0\n0.002972972972972973 0.0\n10\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.0 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n7\n0.0 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.0 0.005945945945945946\n0.0 0.005945945945945946\n0.0 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.0 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n4\n0.005945945945945946 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.0 0.005945945945945946\n0.0 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n3\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.0 0.002972972972972973\n6\n0.002972972972972973 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.005945945945945946 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.0 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.002972972972972973 0.0\n10\n0.005945945945945946 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n5\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.008918918918918918 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.008918918918918918 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n8\n0.005945945945945946 0.0\n0.002972972972972973 0.0\n0.002972972972972973 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.002972972972972973 0.0\n0.002972972972972973 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.0 0.0\n10\n0.002972972972972973 0.0\n0.002972972972972973 0.0\n0.008918918918918918 0.0\n0.0 0.0\n0.008918918918918918 0.0\n0.005945945945945946 0.0\n0.0 0.0\n0.0 0.0\n0.011891891891891892 0.0\n0.002972972972972973 0.0\n10\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.008918918918918918 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n8\n0.005945945945945946 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.0 0.002972972972972973\n0.008918918918918918 0.002972972972972973\n0.014864864864864866 0.002972972972972973\n0.0 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n6\n0.002972972972972973 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.0 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.008918918918918918 0.005945945945945946\n0.008918918918918918 0.005945945945945946\n0.008918918918918918 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.0 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n4\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n8\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.005945945945945946 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n0.002972972972972973 0.002972972972972973\n0.0 0.002972972972972973\n6\n0.002972972972972973 0.0\n0.005945945945945946 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.005945945945945946 0.0\n0.0 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.005945945945945946 0.0\n0.0 0.0\n10\n0.002972972972972973 0.0\n0.002972972972972973 0.0\n0.011891891891891892 0.0\n0.002972972972972973 0.0\n0.0 0.0\n0.005945945945945946 0.0\n0.0 0.0\n0.002972972972972973 0.0\n0.002972972972972973 0.0\n0.005945945945945946 0.0\n10\n0.008918918918918918 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.0 0.005945945945945946\n0.0 0.005945945945945946\n0.0 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.0 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n3\n0.002972972972972973 0.005945945945945946\n0.002972972972972973 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.011891891891891892 0.005945945945945946\n0.0 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.008918918918918918 0.005945945945945946\n0.005945945945945946 0.005945945945945946\n0.0 0.005945945945945946\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-69043b9f2040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmc_shap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-57fd65222417>\u001b[0m in \u001b[0;36mtmc_shap\u001b[0;34m(tolerance, trainFeature, trainLabel, testFeature, testLabel, k, iterations, sources)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maa_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mmarginal_contribs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_score\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mold_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c598f7ab138a>\u001b[0m in \u001b[0;36maa_knn\u001b[0;34m(trainFeature, trainLabel, testFeature, testLabel, k)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0msqV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0msqDis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqDis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    new_value = np.zeros((0,len(trainFeature)))\n",
    "\n",
    "    #new_value,_ = tmc_shap(0.1, trainFeature, trainLabel, testFeature, testLabel, 5, 1, sources=None)\n",
    "    for k in range(10):\n",
    "        value_one,_ = tmc_shap(0.1, trainFeature, trainLabel, testFeature, testLabel, 5, 100, sources=None)\n",
    "        new_value = np.concatenate([new_value,np.reshape(value_one, (1,-1))])\n",
    "        \n",
    "    new_value = np.mean(new_value,0)\n",
    "    print(new_value)\n",
    "    \n",
    "    mod = True\n",
    "\n",
    "    trainLabel_tolist = trainLabel.tolist()\n",
    "    idx_high = int(np.where(trainLabel[i]==np.max(trainLabel[i]))[0])\n",
    "    idx_low = 1 - idx_high\n",
    "\n",
    "    mod_count = 0\n",
    "\n",
    "    while mod:\n",
    "        #a = np.abs(trainLabel[i,0] - trainLabel[i,1])\n",
    "        old_value = new_value\n",
    "\n",
    "        #new_value,_ = tmc_shap(0.3, trainFeature, trainLabel, testFeature, testLabel, 5, 10, sources=None)\n",
    "\n",
    "        trainLabel_tolist[i][idx_high] -= 0.01\n",
    "        trainLabel_tolist[i][idx_low] = 1 - trainLabel_tolist[i][idx_high]\n",
    "\n",
    "        if trainLabel_tolist[i][idx_high] <= 0:\n",
    "            mod = False\n",
    "\n",
    "        trainLabel = array(trainLabel_tolist)\n",
    "        \n",
    "        for k in range(10):\n",
    "            new_value,_ = tmc_shap(0.1, trainFeature, trainLabel, testFeature, testLabel, 5, 100, sources=None)\n",
    "            \n",
    "            print(new_value[i],old_value[i])\n",
    "            if new_value[i] >= old_value[i]:\n",
    "                mod_count += 1\n",
    "\n",
    "        print(mod_count)\n",
    "        if mod_count > 1:\n",
    "            mod_count = 0\n",
    "        else:\n",
    "            mod = False\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.7 0.3]\n"
    }
   ],
   "source": [
    "print(trainLabel[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_iteration(tolerance, trainFeature, trainLabel, testFeature, testLabel, k, sources=None):\n",
    "    if sources is None:\n",
    "        sources = {i: np.array([i]) for i in range(len(trainFeature))}\n",
    "    elif not isinstance(sources, dict):\n",
    "        sources = {i: np.where(sources == i)[0] for i in set(sources)}\n",
    "\n",
    "    idxs = np.random.permutation(len(sources))\n",
    "    marginal_contribs = np.zeros(len(trainFeature))\n",
    "\n",
    "    #x_batch = np.zeros((0,trainFeature.shape[1]))\n",
    "    #y_batch = np.zeros(0,float)\n",
    "\n",
    "    x_batch = trainFeature[idxs[0]]\n",
    "    y_batch = trainLabel[idxs[0]]\n",
    "\n",
    "    truncation_counter = 0\n",
    "\n",
    "    #源代码里面是随机抽取测试数据，进行100次实验获得到的平均socre，这里直接设置为零\n",
    "    new_score = 0\n",
    "\n",
    "    for sh, idx in enumerate(idx):\n",
    "        old_score = new_score\n",
    "\n",
    "        #x_batch = np.concatenate(x_batch, trainLabel[sources[idx]])\n",
    "        #y_batch = np.concatenate(y_batch, trainFeature[sources[idx]])\n",
    "        if sh == 0 :\n",
    "                a=0\n",
    "        else:\n",
    "            x_batch = np.vstack((x_batch, trainFeature[idx]))\n",
    "            y_batch = np.vstack((y_batch, trainLabel[idx]))\n",
    "        if len(set(y_batch)) >= 2:\n",
    "            _,new_score = aa_knn(trainFeature, trainLabel, testFeature, testLabel, k)\n",
    "\n",
    "        marginal_contribs[sources[idx]] = (new_score - old_score)/len(sources[idx])\n",
    "        mean_score = 0.18\n",
    "\n",
    "        distance_to_full_score = np.abs(new_score - mean_score)\n",
    "        if distance_to_full_score <= tolerance * mean_score:\n",
    "            truncation_counter += 1\n",
    "            if truncation_counter > 5:\n",
    "                break\n",
    "        else:\n",
    "            truncation_counter = 0\n",
    "    return marginal_contribs\n",
    "'''\n",
    "        if len(set(y_batch)) >= 2:\n",
    "            new_score = aa_knn()\n",
    "\n",
    "        marginal_contribs[sources[idx]] = (new_score - old_score)/len(sources[idx])\n",
    "        mean_score = 0.18\n",
    "\n",
    "        distance_to_full_score = np.abs(new_score - mean_score)\n",
    "        if distance_to_full_score <= tolerance * mean_score:\n",
    "            truncation_counter += 1\n",
    "            if truncation_counter > 5:\n",
    "                break\n",
    "        else:\n",
    "            truncation_counter = 0\n",
    "    return marginal_contribs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"\n",
    "        for sh, idx in enumerate(idxs):\n",
    "            old_score = new_score\n",
    "\n",
    "            if sh == 0 :\n",
    "                a=0\n",
    "            else:\n",
    "\n",
    "\n",
    "                x_batch = np.vstack((x_batch, trainFeature[idx]))\n",
    "                y_batch = np.vstack((y_batch, trainLabel[idx]))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbasecondaa552d2f6e5da4ad6b3305c7b8c5f25f7",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}